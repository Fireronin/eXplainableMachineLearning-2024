# -*- coding: utf-8 -*-
"""HW1_mbochenek_tosave.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PdQWv8zX-uc6DchOvW25nfwhz2ZcIAtE

# Set up
"""

!pip install tabpfn

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.utils.class_weight import compute_sample_weight

import scipy.optimize as so
from scipy import diag, interp
from itertools import cycle

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

"""# Dataset

For our binary classification problem we selected QSAR-biodegradation dataset [LINK]("https://raw.githubusercontent.com/adrianstando/imbalanced-benchmarking-set/main/datasets/qsar-biodeg.csv"), which has 17 features and 1055 rows. We changes classes labels from {1, 2} to {0, 1} so that we can later use BCELoss while training Neural Network Classifier. Class 0 has 699 instances and class 1 has 356, thus this dataset's imbalance ratio is equal to 1.96.
"""

url = "https://raw.githubusercontent.com/adrianstando/imbalanced-benchmarking-set/main/datasets/qsar-biodeg.csv"
# df = pd.read_csv(url,index_col=0,parse_dates=[0])
df = pd.read_csv(url,index_col=0)
#rename classes from 1 and 2 to 0 and 1
df["TARGET"]-= 1
df

df.groupby("TARGET")["TARGET"].count()

"""We inspected differences between datasets representing two classes with respect to each feature. While most features has similar distribution for both of the classes, some differed significantly for example V14, V30, as presented in the figure below."""

fig, axes = plt.subplots(6,3, figsize=(20,30))

for index, name in enumerate(df.columns[:-1]):
  # print(name, index//3, index%3)
  sns.violinplot(data=df, x="TARGET", y=name, ax=axes[index//3, index%3]);

"""Furthermore we checked how features are correlated. We found that most features had weak correlation whether it was positive or negative one.
The strongest positive correlation equal to 0.92 was observed between V15 and V27, while the strongest negative correlation equal to -0.79 was observed for V18 and V22. The full correlation matrix is presented below.
"""

cm = df[df.columns].corr()
sns.heatmap(cm, square=True, linewidths=.5)

new_cm = cm.copy()
for c in cm.columns:
  new_cm.loc[c, c] = 0

cm_max = new_cm.max().max()
indx = np.where(new_cm == cm_max)[0]
print(f"Strongest positive correlation equal to {round(cm_max, 2)} was observed between {new_cm.index[indx[0]]} and {new_cm.columns[indx[1]]}.")
cm_min = new_cm.min().min()
indx = np.where(new_cm == cm_min)[0]
print(f"Strongest negative correlation equal to {round(cm_min, 2)} was observed between {new_cm.index[indx[0]]} and {new_cm.columns[indx[1]]}.")

"""We split dataset into training and validation datasets 80:20. We perform feature scaling using `sklearn.preprocessing.StandardScaler` as it is known to be good practice for classification tasts. However for tabPFN we will be using uncaled data as should be done according to documentation."""

X = df.loc[:, df.columns != 'TARGET']
Y = df["TARGET"]
# Spliting data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
X_train_tabPFN, X_test_tabPFN= X_train.copy(), X_test.copy()
#scaling - fitting
scaler = StandardScaler().fit(X_train)
#scaling - transforming
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

model_metrics = {"PPV":{},
                 "TRP":{},
                 "ACC":{},
                 "F1":{},
                 "MCC":{},
                 }
predictions = {}
models = {}

"""# Logistic regression

We perform logistic regression using `sklearn.linear_model.LogisticRegression` model using Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) solver.
"""

from sklearn.linear_model import LogisticRegression
model_name = "LR"
model = LogisticRegression(solver = 'lbfgs')

######################################################################################################
#train model
model.fit(X_train, y_train)
#make predictions
y_pred = model.predict(X_test)
#save predictions
predictions[model_name] = y_pred
#save trained model
models[model_name] = model
######################################################################################################

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# SVC
We performed classification using Suppert Vector Machine Classifier (SVC) with radial basis function (RBF) kernel.
"""

from sklearn.svm import SVC
model_name = "SVC"
model = SVC(kernel='rbf')

######################################################################################################
#train model
model.fit(X_train, y_train)
#make predictions
y_pred = model.predict(X_test)
#save predictions
predictions[model_name] = y_pred
#save trained model
models[model_name] = model
######################################################################################################

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# Random Forest Decision Tree (RFDT)

We performed classification with Random Forest Classifier using `sklearn.ensemble.RandomForestClassifier` with `n_estimators = 300` and `max_depth = None`, so that nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
"""

from sklearn.ensemble import RandomForestClassifier
model_name = "RFDT"
model = RandomForestClassifier(n_estimators=300, max_depth=None)

######################################################################################################
#train model
model.fit(X_train, y_train)
#make predictions
y_pred = model.predict(X_test)
#save predictions
predictions[model_name] = y_pred
#save trained model
models[model_name] = model
######################################################################################################

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# Boosting Decision Tree (BDT)

We performed classification using Boosting Decision Tree (BDT) created with `sklearn.ensemble.GradientBoostingClassifier` for `n_estimators = 100`.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
model_name = "BDT"
model = GradientBoostingClassifier(n_estimators=100)

######################################################################################################
#train model
model.fit(X_train, y_train)
#make predictions
y_pred = model.predict(X_test)
#save predictions
predictions[model_name] = y_pred
#save trained model
models[model_name] = model
######################################################################################################

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# Neural Network Classification

We trained `ClassificationNet` that consisted of the following layers:
1. Linear(64, 32) with relu activation
2. Linear(32, 64) with relu activation
3. Linear(64, 1) with sigmoid activation

Then we used BCE Loss for SGD performed for 500 epochs, learning rate = 0.01 and batch size = 64.
"""

import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch.nn import functional as F

class dataset(Dataset):
  def __init__(self,x,y):
    self.x = torch.tensor(x,dtype=torch.float32)
    self.y = torch.tensor(y,dtype=torch.float32)
    self.length = self.x.shape[0]

  def __getitem__(self,idx):
    return self.x[idx],self.y[idx]
  def __len__(self):
    return self.length


class ClassificationNet(nn.Module):
  def __init__(self,input_shape):
    super(ClassificationNet, self).__init__()
    self.input_layer = nn.Linear(input_shape, 32)
    self.hidden_1 = nn.Linear(32, 64)
    self.output_layer = nn.Linear(64, 1)

    #initialization of weights and biases
    for layer in [self.input_layer, self.hidden_1, self.output_layer]:
      nn.init.xavier_uniform_(layer.weight)
      nn.init.zeros_(layer.bias)

  def forward(self,x):
    x = torch.relu(self.input_layer(x))
    x = torch.relu(self.hidden_1(x))
    x = torch.sigmoid(self.output_layer(x))
    return x

BATCH_SIZE = 64
trainset = dataset(X_train, y_train)
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False)

#hyper parameters
learning_rate = 0.01
EPOCHS = 500
# Model , Optimizer, Loss
model = ClassificationNet(input_shape=X_train.shape[1])
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
loss_fn = nn.BCELoss()

losses = []
accuracy = []
for epoch in range(EPOCHS):
  for i, data in enumerate(trainloader):
    inputs, labels = data
    # Zero gradients for every batch!
    optimizer.zero_grad()
    #calculate output
    output = model(inputs)
    #calculate loss
    loss = loss_fn(output, labels.reshape(-1,1))
    #accuracy
    predicted = model(torch.tensor(X_test, dtype=torch.float32))
    acc = (predicted.reshape(-1).detach().numpy().round() == y_test).mean()
    #backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  if epoch%50 == 0:
    losses.append(loss)
    accuracy.append(acc)
    print("epoch {}\tloss : {}\t accuracy : {}".format(epoch, loss, acc))

from google.colab import drive
drive.mount('/content/drive')
PATH = '/content/drive/My Drive/Colab Notebooks/EML/HW1/classifier_network.pt'
torch.save(model.state_dict(), PATH)

model_name = "NNC"
models[model_name] = model
#predict
y_pred = model(torch.tensor(X_test, dtype=torch.float32))
y_pred = y_pred.reshape(-1).detach().numpy().round()
#save predictions
predictions[model_name] = y_pred
######################################################################################################

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# TabPFN

We performed classification using tabPFN classifier with `N_ensemble_configurations=18`.
"""

from tabpfn import TabPFNClassifier
model_name = "tabPFN"

# N_ensemble_configurations controls the number of model predictions that are ensembled with feature and class rotations (See our work for details).
# When N_ensemble_configurations > #features * #classes, no further averaging is applied.

classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=18)

classifier.fit(X_train_tabPFN, y_train)
y_pred, p_eval = classifier.predict(X_test_tabPFN, return_winning_probability=True)

#save
models[model_name] = classifier
predictions[model_name] = y_pred
print('Accuracy', accuracy_score(y_test, y_pred))

######################################################################################################
model_metrics["PPV"][model_name] = metrics.precision_score(y_test, y_pred)
model_metrics["TRP"][model_name] = metrics.recall_score(y_test, y_pred)
model_metrics["ACC"][model_name] = metrics.accuracy_score(y_test, y_pred)
model_metrics["F1"][model_name] = metrics.f1_score(y_test, y_pred)
model_metrics["MCC"][model_name] = metrics.matthews_corrcoef(y_test, y_pred)

for metric in model_metrics:
  print(f"{metric}: {round(model_metrics[metric][model_name], 2)}")
######################################################################################################
print(classification_report(y_test, y_pred))
print("\n")
######################################################################################################
#Normalized confusion matrix
metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true');
plt.show()
######################################################################################################

"""# Comparing results

We used following metrics to compare models performance:  

* Accuracy ($ACC$),
* Precision ($PPV$),
* Sensivity ($TRP$),
* F1 score ($F1$),
* Matthews correlation coefficient ($MCC$).

All of the results are presented in the table below.

The Random Forest Decision Tree (RFDT) and Neural Network Classifier (NNC) had the highest accuracy equal to 0.86. While the Boosting DEcision Tree (BDT) had highest sensitivity among all models (0.92), the NNC had the highest precision (0.74).

Overall selected models can be divided into two similarly performing groups:
1. better performing: RFDT, BDT, NNC ($ACC \approx 0.87$)
2. models with slightly worse performance: Linear Reagression (LR), Suppert Vector Machine Classifier (SVC) and tabPFN.

Moreover, this division can be infered from ROC curve analysis, which is presented in the figure below. The BDT has the highest AUC (area under curve) value equal to 0.879, while tabPFN has the lowest equal to 0.812.

This shows that decision tree models as well as neural networks seem to better understand and describe patterns present in dataset, thus yield better classification performance.
"""

df_metrics = pd.DataFrame(model_metrics)
df_metrics = df_metrics.T
df_metrics = df_metrics.loc[["ACC", "PPV", "TRP", "F1", "MCC"], :]
df_metrics.round(2)

fpr, tpr, thresholds = roc_curve(y_test, predictions["LR"])
fig, ax = plt.subplots(1, 1, figsize=(10, 9))
for model in predictions:
  pred = predictions[model]
  fpr, tpr, thresholds = roc_curve(y_test, pred)
  roc_auc = round(auc(fpr, tpr), 3)
  ax.plot(fpr, tpr, lw=2, alpha = 0.7, label=f"{model}, AUC = {roc_auc}")

ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve', fontsize=15)
plt.legend(loc="lower right", fontsize=12)
plt.show()
